{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Invalid alias: The name clear can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name more can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name less can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name man can't be aliased because it is another magic command.\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "from utils.loss import dice_loss\n",
    "from utils.dataloader import Dataset_3D\n",
    "import utils.alienlab as alien\n",
    "from utils.ply import write_ply\n",
    "import numpy as np\n",
    "from utils.models import ResNetUNet, my_model_simple\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import utils.vox_to_coord as vtc\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_names = ['background', 'flowers', 'peduncle', 'stem', 'leaves', 'fruits']\n",
    "N_cam = 72\n",
    "B = 6\n",
    "path_imgs = \"data/arabidopsis/\"\n",
    "\n",
    "Sx = 896 #Center crop\n",
    "Sy = 448\n",
    "xinit = 1080 #Original image size\n",
    "yinit = 1920\n",
    "label_num = 6\n",
    "reduction_factor = 18\n",
    "\n",
    "N_vox = 2197000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
    "\n",
    "    pred = F.sigmoid(pred)\n",
    "    dice = dice_loss(pred, target)\n",
    "\n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "\n",
    "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):\n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "\n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def train_model(model, optimizer, scheduler, criterium, num_epochs=25):\n",
    "    xy_full_flat = torch.load('voxel_coord/coordinates_full_pipe.pt').to(device)    \n",
    "    pred_pad = torch.zeros((N_cam//reduction_factor, label_num, xinit, yinit)).to(device)\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    \n",
    "\n",
    "                    pred_pad[:,:,(xinit-Sx)//2:(xinit+Sx)//2,\n",
    "                             (yinit-Sy)//2:(yinit+Sy)//2] = outputs #To fit the camera parameters\n",
    "\n",
    "                    preds_flat = pred_pad.permute(0,2,3,1)\n",
    "                    preds_flat = vtc.adjust_predictions(preds_flat)\n",
    "\n",
    "\n",
    "                    assign_preds = preds_flat[xy_full_flat].reshape(pred_pad.shape[0], \n",
    "                                                    xy_full_flat.shape[0]//pred_pad.shape[0], preds_flat.shape[-1])\n",
    "\n",
    "                    assign_preds[:,:,6] = 0\n",
    "                    assign_preds = torch.sum(assign_preds, dim = 0)\n",
    "\n",
    "                    loss = criterium(assign_preds, labels[0])\n",
    "                    print(loss)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(\"saving best model\")\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection of coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    \n",
    "    with open('images.json', 'r') as f:\n",
    "        pose = json.load(f)\n",
    "\n",
    "    N_cam = 72\n",
    "    N_feat = 72\n",
    "\n",
    "    extrinsics = torch.zeros((N_cam, 3, 4))\n",
    "    for i in range(N_cam):\n",
    "        rot = pose[str(i+1)]['rotmat']\n",
    "        extrinsics[i][:3,:3] = torch.Tensor(rot)\n",
    "        trans = pose[str(i+1)]['tvec']\n",
    "        extrinsics[i][:,3] = torch.Tensor(trans)/10.\n",
    "\n",
    "    with open('cameras.json', 'r') as f:\n",
    "        focal = json.load(f)\n",
    "    focal = focal['1']['params']\n",
    "\n",
    "    r = 1#1080/448\n",
    "\n",
    "    intrinsics = torch.zeros((1, 3, 3))\n",
    "    intrinsics[:,0,0] = focal[0]*r\n",
    "    intrinsics[:,1,1] = focal[0]*r\n",
    "    intrinsics[:,0,2] = focal[1]*r\n",
    "    intrinsics[:,1,2] = focal[2]*r\n",
    "    intrinsics[:,2,2] = 1\n",
    "    \n",
    "    cloud_scale = 0.5\n",
    "    #pred_pad = pred_pad.permute(0,2,3,1)\n",
    "    #the_shape = pred_pad.shape\n",
    "    #del pred_pad\n",
    "    \n",
    "\n",
    "    N = int(65/cloud_scale)\n",
    "    #Voxel representation of the point cloud\n",
    "    min_vec = [int(-40/cloud_scale), int(-40/cloud_scale),int(-5/cloud_scale)] #Limit of the cloud\n",
    "    basis_voxels = vtc.basis_vox(min_vec, N, N, N)*cloud_scale#List of coordinates \n",
    "\n",
    "    #Camera projection\n",
    "    torch_voxels = torch.from_numpy(basis_voxels)\n",
    "\n",
    "    #Perspective projection\n",
    "    xy_coords = vtc.project_coordinates(torch_voxels, intrinsics, extrinsics, give_prod = False)\n",
    "\n",
    "    #permute x and y coordinates\n",
    "    xy_coords[:, 2, :] = xy_coords[:,0,:]\n",
    "    xy_coords[:, 0, :] = xy_coords[:,1,:]\n",
    "    xy_coords[:, 1, :] = xy_coords[:,2,:]\n",
    "\n",
    "    coords = vtc.correct_coords_outside(xy_coords, Sx, Sy, xinit, yinit, -1) #correct the coordinates that project outside\n",
    "    coords = coords[::reduction_factor]\n",
    "    xy_full_flat = vtc.flatten_coordinates(coords, torch.Size([N_cam//reduction_factor, Sx, Sy, label_num]))\n",
    "    torch.save(xy_full_flat, 'voxel_coord/coordinates_full_pipe.pt')\n",
    "    torch.save(torch_voxels, 'voxel_coord/voxels_full_pipe.pt')\n",
    "    \n",
    "    del torch_voxels\n",
    "    del xy_full_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_3D_set(mode, path = path_imgs, N_subsample = 1, N_cam = N_cam):\n",
    "    if mode not in ['train', 'val', 'test']:\n",
    "    \n",
    "        print(\"mode should be 'train', 'val' or 'test'\")\n",
    "        \n",
    "    image_paths = np.sort(glob.glob(path + mode + \"/images/*.png\"))\n",
    "    target_paths = np.sort(glob.glob(path + mode + \"/3D_label/*.pt\"))\n",
    "    target_paths = np.repeat(target_paths, N_cam)\n",
    "\n",
    "    return  image_paths[::N_subsample], target_paths[::N_subsample]\n",
    "\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    transforms.CenterCrop((896, 448)),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "\n",
    "image_paths, target_paths = init_3D_set('train', N_subsample = reduction_factor)\n",
    "\n",
    "image_val, target_val = init_3D_set('val', N_subsample = reduction_factor)\n",
    "\n",
    "image_test, target_test = init_3D_set('test',N_subsample = reduction_factor)\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "train_dataset = Dataset_3D(image_paths, target_paths, transform=trans)\n",
    "val_dataset = Dataset_3D(image_val, target_val, transform=trans)\n",
    "test_dataset = Dataset_3D(image_test, target_test, transform=trans)\n",
    "\n",
    "\n",
    "image_datasets = {\n",
    "        'train': train_dataset, 'val': val_dataset , 'test': test_dataset\n",
    "        }\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0),\n",
    "    'val': DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0),\n",
    "    'test': DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Epoch 0/4\n",
      "----------\n",
      "LR 0.0001\n",
      "tensor(1.9419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9327, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b8ff566e93b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mcriterium\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterium\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_weights/attempt_model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b28ee3ba40c8>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, scheduler, criterium, num_epochs)\u001b[0m\n\u001b[1;32m     81\u001b[0m                     \u001b[0;31m# backward + optimize only if in training phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Segmentation/segmentation/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Segmentation/segmentation/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_class = 6\n",
    "model = ResNetUNet(num_class).to(device)\n",
    "\n",
    "# freeze backbone layers\n",
    "for l in model.base_layers:\n",
    "    for param in l.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=30, gamma=0.1)\n",
    "\n",
    "xy_full_flat = torch.load('voxel_coord/coordinates_05_shift6.pt')\n",
    "xy_full_flat = xy_full_flat.reshape(N_vox, N_cam)\n",
    "xy_full_flat = xy_full_flat[:,::reduction_factor]\n",
    "\n",
    "\n",
    "wb = 5/1e6\n",
    "wc = 1/1e4\n",
    "weights = [wb, wc, wc, wc, wc, wc, wb] #[ 1 / number of instances for each class]\n",
    "class_weights = torch.FloatTensor(weights).cuda()\n",
    "\n",
    "#criterium = nn.NLLLoss(weights = class_weights)\n",
    "criterium = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "model = train_model(model, optimizer_ft, exp_lr_scheduler, criterium, num_epochs=5)\n",
    "torch.save(model, 'model_weights/attempt_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully trained pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 2197000, 7])\n",
      "torch.Size([1, 2197000, 72, 7]) torch.Size([1, 1, 72, 7])\n",
      "torch.Size([1, 2197000, 72, 7])\n",
      "torch.Size([1, 2197000, 1, 7])\n",
      "torch.Size([3773, 4])\n",
      "torch.Size([72, 2197000, 7])\n",
      "torch.Size([1, 2197000, 72, 7]) torch.Size([1, 1, 72, 7])\n",
      "torch.Size([1, 2197000, 72, 7])\n",
      "torch.Size([1, 2197000, 1, 7])\n",
      "torch.Size([3765, 4])\n",
      "torch.Size([72, 2197000, 7])\n",
      "torch.Size([1, 2197000, 72, 7]) torch.Size([1, 1, 72, 7])\n",
      "torch.Size([1, 2197000, 72, 7])\n",
      "torch.Size([1, 2197000, 1, 7])\n",
      "torch.Size([3714, 4])\n",
      "torch.Size([72, 2197000, 7])\n",
      "torch.Size([1, 2197000, 72, 7]) torch.Size([1, 1, 72, 7])\n",
      "torch.Size([1, 2197000, 72, 7])\n",
      "torch.Size([1, 2197000, 1, 7])\n",
      "torch.Size([3771, 4])\n",
      "torch.Size([72, 2197000, 7])\n",
      "torch.Size([1, 2197000, 72, 7]) torch.Size([1, 1, 72, 7])\n",
      "torch.Size([1, 2197000, 72, 7])\n",
      "torch.Size([1, 2197000, 1, 7])\n",
      "torch.Size([3761, 4])\n",
      "torch.Size([72, 2197000, 7])\n",
      "torch.Size([1, 2197000, 72, 7]) torch.Size([1, 1, 72, 7])\n",
      "torch.Size([1, 2197000, 72, 7])\n",
      "torch.Size([1, 2197000, 1, 7])\n",
      "torch.Size([3810, 4])\n",
      "torch.Size([72, 2197000, 7])\n",
      "torch.Size([1, 2197000, 72, 7]) torch.Size([1, 1, 72, 7])\n",
      "torch.Size([1, 2197000, 72, 7])\n",
      "torch.Size([1, 2197000, 1, 7])\n",
      "torch.Size([3819, 4])\n",
      "torch.Size([72, 2197000, 7])\n",
      "torch.Size([1, 2197000, 72, 7]) torch.Size([1, 1, 72, 7])\n",
      "torch.Size([1, 2197000, 72, 7])\n",
      "torch.Size([1, 2197000, 1, 7])\n",
      "torch.Size([3784, 4])\n"
     ]
    }
   ],
   "source": [
    "reduction_factor = 1\n",
    "image_paths, target_paths = init_3D_set('train', N_subsample = reduction_factor)\n",
    "\n",
    "batch_size = 4\n",
    "N_views = 72\n",
    "n = 9\n",
    "\n",
    "train_dataset = Dataset_3D(image_paths[N_views*n:N_views*(n+1)], target_paths[N_views*n:N_views*(n+1)], \n",
    "                           transform=trans)\n",
    "\n",
    "loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "model_segmentation = torch.load('model_weights/unet_448_long_train.pt') #needs utils.models imported\n",
    "model_classification = torch.load(\"classification_trials/shifting_blur1_weight_classview_bias_none_lr_005_wb_1e5_wc_1e4.pt\")\n",
    "with torch.no_grad():\n",
    "\n",
    "\n",
    "    pred_tot = []\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model_segmentation(inputs)\n",
    "        pred_tot.append(outputs)\n",
    "\n",
    "    pred_tot = torch.cat(pred_tot, dim = 0) #All predictions into one tensor\n",
    "    pred_tot_class = pred_tot[:,1:,:,:]\n",
    "    #pred_tot_class = nn.MaxPool2d(sf*2)(pred_tot_class)\n",
    "    #pred_tot_class = nn.Upsample(scale_factor = sf*2)(pred_tot_class)\n",
    "    pred_tot[:,1:,:,:] = pred_tot_class\n",
    "\n",
    "    pred_pad = torch.zeros((N_cam, label_num, xinit, yinit))\n",
    "    Sbix = pred_tot.shape[2]\n",
    "    Sbiy = pred_tot.shape[3]\n",
    "    pred_pad[:,:,(xinit-Sbix)//2:(xinit+Sbix)//2,(yinit-Sbiy)//2:(yinit+Sbiy)//2] = pred_tot #To fit the camera parameters\n",
    "    del pred_tot\n",
    "    pred_pad = pred_pad.permute(0,2,3,1)\n",
    "\n",
    "    preds_flat = vtc.adjust_predictions(pred_pad)\n",
    "    \n",
    "    total_points = []\n",
    "    for i in range(8):\n",
    "        xy_full_flat = torch.load('voxel_coord/coordinates_05_shift%d.pt'%i).to(device)\n",
    "        assign_preds = preds_flat[xy_full_flat].reshape(pred_pad.shape[0], \n",
    "                                                        xy_full_flat.shape[0]//pred_pad.shape[0], preds_flat.shape[-1])\n",
    "        del xy_full_flat\n",
    "        #assign_preds = assign_preds[:,:,:-1]\n",
    "        #assign_preds = torch.sum(assign_preds, dim =  0)\n",
    "        assign_preds = assign_preds.permute(1, 0, 2).unsqueeze(0)\n",
    "        assign_preds = model_classification[0](assign_preds.to(device))\n",
    "        assign_preds = torch.argmax(assign_preds[0, :, :-1], dim = 1)\n",
    "        vol = torch.load('voxel_coord/voxels_05_shift%d.pt'%i)\n",
    "        ind = (assign_preds != 0)*(assign_preds != 4)\n",
    "        vol = vol[ind]\n",
    "        vol[:,3] = assign_preds[ind]\n",
    "        total_points.append(vol)\n",
    "        del vol\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_points = torch.cat(total_points, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_ply('high_quality_class.ply', total_points.detach().cpu().numpy(),\n",
    "      ['x', 'y', 'z', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2197000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assign_preds = torch.argmax(assign_preds, dim = 1)assign_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2197000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vol' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7d64a3b2b8fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vol' is not defined"
     ]
    }
   ],
   "source": [
    "vol.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
