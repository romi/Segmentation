#!/usr/bin/env python3

import sys

sys.path.append("/home/alienor/Documents/Segmentation/")

import luigi

from romidata.task import RomiTask
from romiscan import tasks
from romidata import io
import json
import torch
import numpy as np
from SegmentationReconstruction import segmentation_space_carving


import importlib
import os

from romidata.task import ImagesFilesetExists, RomiTask, FileByFileTask
from romidata import io

from romiscan.filenames import *
from romiscan import arabidopsis
from romiscan import proc2d
from romiscan import proc3d
from romiscan import cl
from romiscan import colmap
from romiscan.tasks import *
import open3d



class SegmentationReconstruction(RomiTask):
    """Backproject masks into 3D space
    """
    upstream_task = None
    upstream_image = luigi.TaskParameter(default=Undistorted)
    upstream_colmap = luigi.TaskParameter(default=Colmap)
  
     
    label_names = luigi.ListParameter(default=['background', 'flowers', 'peduncle', 'stem', 'leaves', 'fruits'])
    Sx = luigi.IntParameter(default=896)
    Sy = luigi.IntParameter(default=896)
    cloud_scale = luigi.IntParameter(default=4)

    def requires(self):
        return {'images': self.upstream_image(), 'colmap': self.upstream_colmap()}
    
    def read_intrinsics(self, scan):
        """
        Read camera intrinsics
        input: scan id fsdb
        output: -xinit, yinit [int] image height and width
                -intrinsics [torch tensor] 3x3 matrix of camera intrinsics: focal and optical center position on images
        """
        #search metada for camera parameters, contained in a "camera.json" file in the scan folder
        try:
            camera_model = scan.get_metadata()['computed']['camera_model']
        except:
            camera_model = scan.get_metadata()['scanner']['camera_model']
        if camera_model is None:
            raise Exception("Could not find camera model for Backprojection")
            
        #image dimensions         
        xinit = camera_model['height'] #image width
        yinit = camera_model['width'] #image height
        
        focal = camera_model['params'][0:4] #camera intrinsics
        
        #buiding the extrinsics matrix
        intrinsics = torch.zeros((1, 3, 3))
        intrinsics[:,0,0] = focal[0]        
        intrinsics[:,1,1] = focal[1]
        intrinsics[:,0,2] = focal[2]
        intrinsics[:,1,2] = focal[3]
        intrinsics[:,2,2] = 1
        
        return xinit, yinit, intrinsics
    
    def read_extrinsics(self, colmap_fileset, images_fileset,N_cam):
        """
        Read camera extrinsics
        input: scan id fsdb
        output: -extrinsics [torch tensor] 3x4 matrix of camera extrinsics:
            rotation and translation of the object with respoect to the camera
        """
        
        #Collect camera positions, there are as many as images in the scan folder. Located in "images.json" file
        poses = io.read_json(colmap_fileset.get_file('images'))

        #camera extrinsics
        extrinsics = torch.zeros((N_cam, 3, 4))
        im_num = 0
        
        #Associate camera position to corresponding image
        for fi in images_fileset:
            key = None
            for k in poses.keys():
                if os.path.splitext(poses[k]['name'])[0] == fi.id:
                    key = k
                    break

            if key is not None:
                rot = poses[key]['rotmat']
                tvec = poses[key]['tvec']
                extrinsics[im_num][:3,:3] = torch.Tensor(rot)
                extrinsics[im_num][:,3] = torch.Tensor(tvec)
            im_num +=1
        
        return extrinsics
    
    def limit_carving_volume(self, colmap_fileset):
        """
        Using the sparse cloud generated by colmap^to find the camera positions, we define the limits
        of the volume to carve for space carving
        in: colmpa fileset
        out:    -min_vox: [np array] 3 coordinates origin of the volume to carve
                -num_vox: [np array] number of voxels in X, Y, Z direction
        """
        
        pcd = io.read_point_cloud(colmap_fileset.get_file(COLMAP_SPARSE_ID))

#Get limits of the volume to carve using the sparse points computed by colmap
        points = np.asarray(pcd.points)

        min_vox = points.min(axis=0)//self.cloud_scale
        
        num_vox = (points.max(axis=0) - points.min(axis=0))//(self.cloud_scale) + 1
        num_vox = num_vox.astype(int)
        return min_vox, num_vox


    def run(self):
        
        images_fileset = self.input()['images'].get().get_files()
        colmap_fileset = self.input()['colmap'].get()

        scan = colmap_fileset.scan
        N_cam = len(images_fileset)

        #PINHOLE MODEL
        xinit, yinit, intrinsics = self.read_intrinsics(scan)
        extrinsics = self.read_extrinsics(colmap_fileset, images_fileset, N_cam)
        #LIMITS OF VOLUME TO CARVE
        min_vox, num_vox = self.limit_carving_volume(colmap_fileset)
   
        #SPACE CARVING
        vol = segmentation_space_carving(extrinsics, intrinsics, 
                                   min_vox, num_vox, N_cam, 
                                   self.Sx, self.Sy, xinit, yinit, self.cloud_scale, self.label_names,
                                   images_fileset, scan)

        #WRITE OUTPUT
        pcd = open3d.geometry.PointCloud()
        pcd.points = open3d.utility.Vector3dVector(vol[:,:3])
        outfile = self.output_file()
        io.write_point_cloud(outfile, pcd)
        outfile.set_metadata({'voxel_size' : self.cloud_scale, 'origin' : min_vox.tolist(), 'labels' : vol[:,3].tolist() })

        
if __name__ == "__main__":
    luigi.run()