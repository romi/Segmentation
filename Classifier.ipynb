{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Invalid alias: The name clear can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name more can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name less can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name man can't be aliased because it is another magic command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "from torch.optim import lr_scheduler\n",
    "import copy\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We read the dataset and create an iterable.\n",
    "class features_from_views(data.Dataset):\n",
    "    def __init__(self, features_path, gt_path):\n",
    "        \n",
    "        self.features = features_path\n",
    "        self.ground_truth = gt_path\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):   # Function that returns one point and one label.\n",
    "        self.feat = torch.load(self.features[index])\n",
    "        self.feat = self.feat.permute(1, 0, 2)\n",
    "        self.feat = torch.flatten(self.feat, start_dim = 1)\n",
    "        self.gt = torch.load(self.ground_truth[index])\n",
    "        self.gt = torch.sparse.FloatTensor(self.gt[0].unsqueeze(0), self.gt[1], \n",
    "                              size = torch.Size([self.feat.shape[0]])).to_dense()\n",
    "        return  self.feat, self.gt\n",
    "    \n",
    "    def __len__(self):   # Length of the dataset.\n",
    "        return len(self.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the dataloader.\n",
    "path_features = 'LEARNING/features/*.pt'\n",
    "path_ground_truth = 'LEARNING/ground_truth_3D/*.pt'\n",
    "list_features = glob.glob(path_features)\n",
    "list_ground_truth = glob.glob(path_ground_truth)\n",
    "train_features = features_from_views(list_features, list_ground_truth)\n",
    "batch_size = 1\n",
    "feature_loader = data.DataLoader(train_features, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for feat, gt in feature_loader:\n",
    "#    print(feat.shape, gt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build a simple model with the inputs and one output layer.\n",
    "class my_model(nn.Module):\n",
    "    def __init__(self,n_in = 504,n_hidden=10,n_out= 6 ):\n",
    "        super(my_model,self).__init__()\n",
    "        self.n_in  = n_in\n",
    "        self.n_out = n_out\n",
    "         \n",
    "        self.linearlinear = nn.Sequential(\n",
    "            nn.Linear(self.n_in,self.n_out,bias=True),   # Hidden layer.\n",
    "            )\n",
    "       # self.logprob = nn.LogSoftmax(dim=1)                 # -Log(Softmax probability).\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.linearlinear(x)\n",
    "       # x = self.logprob(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we create the mode, the loss function or criterium and the optimizer \n",
    "# that we are going to use to minimize the loss.\n",
    "\n",
    "# Model.\n",
    "model = my_model().to(device)\n",
    "\n",
    "# Negative log likelihood loss.\n",
    "#criterium = nn.NLLLoss()\n",
    "criterium = nn.CrossEntropyLoss()\n",
    "#criterium = my_loss()\n",
    "# Adam optimizer with learning rate 0.1 and L2 regularization with weight 1e-4.\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.1,weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.98023224e-08, 4.24460070e-08, 6.04537956e-08, 8.61014184e-08,\n",
       "       1.22630088e-07, 1.74656105e-07, 2.48754246e-07, 3.54288647e-07,\n",
       "       5.04596191e-07, 7.18671960e-07, 1.02356973e-06, 1.45782088e-06,\n",
       "       2.07630380e-06, 2.95717912e-06, 4.21176725e-06, 5.99861647e-06,\n",
       "       8.54353943e-06, 1.21681502e-05, 1.73305080e-05, 2.46830047e-05,\n",
       "       3.51548102e-05, 5.00692966e-05, 7.13112785e-05, 1.01565206e-04,\n",
       "       1.44654413e-04, 2.06024287e-04, 2.93430432e-04, 4.17918778e-04,\n",
       "       5.95221511e-04, 8.47745221e-04, 1.20740253e-03, 1.71964506e-03,\n",
       "       2.44920731e-03, 3.48828757e-03, 4.96819934e-03, 7.07596613e-03,\n",
       "       1.00779565e-02, 1.43535462e-02, 2.04430619e-02, 2.91160647e-02,\n",
       "       4.14686033e-02, 5.90617268e-02, 8.41187620e-02, 1.19806286e-01,\n",
       "       1.70634301e-01, 2.43026185e-01, 3.46130446e-01, 4.92976861e-01,\n",
       "       7.02123109e-01, 1.00000000e+00])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = np.linspace(-25, 0, 50)\n",
    "2**ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taining.\n",
    "for k, (data, target) in enumerate(feature_loader):\n",
    "    # Definition of inputs as variables for the net.\n",
    "    # requires_grad is set False because we do not need to compute the \n",
    "    # derivative of the inputs.\n",
    "    data   = Variable(data,requires_grad=False)\n",
    "    target = Variable(target.long(),requires_grad=False)\n",
    "    print(data.shape, target.shape)\n",
    "    # Set gradient to 0.\n",
    "    optimizer.zero_grad()\n",
    "    # Feed forward.\n",
    "    pred = model(data)\n",
    "    pred = pred.permute(0, 2, 1)\n",
    "    # Loss calculation.\n",
    "    print(pred.max())\n",
    "    loss = criterium(pred,target)\n",
    "    # Gradient calculation.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Print loss every 10 iterations.\n",
    "    if k%10==0:\n",
    "        print('Loss {:.4f} at iter {:d}'.format(loss.item(),k))\n",
    "        \n",
    "    # Model weight modification based on the optimizer. \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, scheduler, num_epochs=25):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "            i = -25.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.param_groups[0]['lr'] = 2**i\n",
    "                i = i + 1\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    outputs = outputs.permute(0, 2, 1)\n",
    "                    #loss = calc_loss(outputs, labels, metrics)\n",
    "                    loss = criterium(outputs, labels)\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        train_loss.append(loss.detach().cpu().numpy())\n",
    "                    else: \n",
    "                        val_loss.append(loss.detach().cpu().numpy())\n",
    "                    print(loss, optimizer.param_groups[0]['lr'])\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "            print(loss)\n",
    "            #print_metrics(metrics, epoch_samples, phase)\n",
    "            #epoch_loss = metrics['loss'] / epoch_samples\n",
    "\n",
    "            # deep copy the model\n",
    "            #if phase == 'val' and epoch_loss < best_loss:\n",
    "            #    print(\"saving best model\")\n",
    "            #    best_loss = epoch_loss\n",
    "            #    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    print('Best val loss: {:4f}'.format(loss))\n",
    "\n",
    "    # load best model weights\n",
    "    #model.load_state_dict(best_model_wts)\n",
    "    return model, val_loss, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Epoch 0/0\n",
      "----------\n",
      "LR 0.1\n",
      "tensor(2.0765, device='cuda:0', grad_fn=<NllLoss2DBackward>) 2.9802322387695312e-08\n",
      "tensor(2.0792, device='cuda:0', grad_fn=<NllLoss2DBackward>) 5.960464477539063e-08\n",
      "tensor(2.0755, device='cuda:0', grad_fn=<NllLoss2DBackward>) 1.1920928955078125e-07\n",
      "tensor(2.0783, device='cuda:0', grad_fn=<NllLoss2DBackward>) 2.384185791015625e-07\n",
      "tensor(2.0727, device='cuda:0', grad_fn=<NllLoss2DBackward>) 4.76837158203125e-07\n",
      "tensor(2.0848, device='cuda:0', grad_fn=<NllLoss2DBackward>) 9.5367431640625e-07\n",
      "tensor(2.0786, device='cuda:0', grad_fn=<NllLoss2DBackward>) 1.9073486328125e-06\n",
      "tensor(2.0763, device='cuda:0', grad_fn=<NllLoss2DBackward>) 3.814697265625e-06\n",
      "tensor(2.0840, device='cuda:0', grad_fn=<NllLoss2DBackward>) 7.62939453125e-06\n",
      "tensor(2.0735, device='cuda:0', grad_fn=<NllLoss2DBackward>) 1.52587890625e-05\n",
      "tensor(2.0810, device='cuda:0', grad_fn=<NllLoss2DBackward>) 3.0517578125e-05\n",
      "tensor(2.0634, device='cuda:0', grad_fn=<NllLoss2DBackward>) 6.103515625e-05\n",
      "tensor(2.0598, device='cuda:0', grad_fn=<NllLoss2DBackward>) 0.0001220703125\n",
      "tensor(2.0488, device='cuda:0', grad_fn=<NllLoss2DBackward>) 0.000244140625\n",
      "tensor(2.0097, device='cuda:0', grad_fn=<NllLoss2DBackward>) 0.00048828125\n",
      "tensor(1.9541, device='cuda:0', grad_fn=<NllLoss2DBackward>) 0.0009765625\n",
      "tensor(1.8265, device='cuda:0', grad_fn=<NllLoss2DBackward>) 0.001953125\n",
      "tensor(1.5973, device='cuda:0', grad_fn=<NllLoss2DBackward>) 0.00390625\n",
      "tensor(1.1660, device='cuda:0', grad_fn=<NllLoss2DBackward>) 0.0078125\n",
      "tensor(0.5501, device='cuda:0', grad_fn=<NllLoss2DBackward>) 0.015625\n",
      "tensor(0.0883, device='cuda:0', grad_fn=<NllLoss2DBackward>) 0.03125\n",
      "tensor(0.0242, device='cuda:0', grad_fn=<NllLoss2DBackward>) 0.0625\n",
      "tensor(0.0264, device='cuda:0', grad_fn=<NllLoss2DBackward>) 0.125\n",
      "tensor(0.0696, device='cuda:0', grad_fn=<NllLoss2DBackward>) 0.25\n",
      "tensor(0.0772, device='cuda:0', grad_fn=<NllLoss2DBackward>) 0.5\n",
      "tensor(0.2641, device='cuda:0', grad_fn=<NllLoss2DBackward>) 1.0\n",
      "tensor(0.4616, device='cuda:0', grad_fn=<NllLoss2DBackward>) 2.0\n",
      "tensor(0.5091, device='cuda:0', grad_fn=<NllLoss2DBackward>) 4.0\n",
      "tensor(1.2269, device='cuda:0', grad_fn=<NllLoss2DBackward>) 8.0\n",
      "tensor(1.5071, device='cuda:0', grad_fn=<NllLoss2DBackward>) 16.0\n",
      "tensor(4.5609, device='cuda:0', grad_fn=<NllLoss2DBackward>) 32.0\n",
      "tensor(5.1934, device='cuda:0', grad_fn=<NllLoss2DBackward>) 64.0\n",
      "tensor(10.2229, device='cuda:0', grad_fn=<NllLoss2DBackward>) 128.0\n",
      "tensor(8.0238, device='cuda:0', grad_fn=<NllLoss2DBackward>) 256.0\n",
      "tensor(22.1620, device='cuda:0', grad_fn=<NllLoss2DBackward>) 512.0\n",
      "tensor(74.1000, device='cuda:0', grad_fn=<NllLoss2DBackward>) 1024.0\n",
      "tensor(64.7576, device='cuda:0', grad_fn=<NllLoss2DBackward>) 2048.0\n",
      "tensor(113.9874, device='cuda:0', grad_fn=<NllLoss2DBackward>) 4096.0\n",
      "tensor(311.7171, device='cuda:0', grad_fn=<NllLoss2DBackward>) 8192.0\n",
      "tensor(1561.3085, device='cuda:0', grad_fn=<NllLoss2DBackward>) 16384.0\n",
      "tensor(591.0389, device='cuda:0', grad_fn=<NllLoss2DBackward>) 32768.0\n",
      "tensor(90262.7109, device='cuda:0', grad_fn=<NllLoss2DBackward>) 65536.0\n",
      "tensor(325466.5625, device='cuda:0', grad_fn=<NllLoss2DBackward>) 131072.0\n",
      "tensor(759717.6875, device='cuda:0', grad_fn=<NllLoss2DBackward>) 262144.0\n",
      "tensor(6210178., device='cuda:0', grad_fn=<NllLoss2DBackward>) 524288.0\n",
      "tensor(625321.8750, device='cuda:0', grad_fn=<NllLoss2DBackward>) 1048576.0\n",
      "tensor(17532452., device='cuda:0', grad_fn=<NllLoss2DBackward>) 2097152.0\n",
      "tensor(9644332., device='cuda:0', grad_fn=<NllLoss2DBackward>) 4194304.0\n",
      "tensor(87276224., device='cuda:0', grad_fn=<NllLoss2DBackward>) 8388608.0\n",
      "tensor(15351650., device='cuda:0', grad_fn=<NllLoss2DBackward>) 16777216.0\n",
      "tensor(15351650., device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "tensor(4.4463e+08, device='cuda:0') 2.9802322387695312e-08\n",
      "tensor(4.4515e+08, device='cuda:0') 5.960464477539063e-08\n",
      "tensor(4.4480e+08, device='cuda:0') 1.1920928955078125e-07\n",
      "tensor(4.4595e+08, device='cuda:0') 2.384185791015625e-07\n",
      "tensor(4.4314e+08, device='cuda:0') 4.76837158203125e-07\n",
      "tensor(4.4848e+08, device='cuda:0') 9.5367431640625e-07\n",
      "tensor(4.4737e+08, device='cuda:0') 1.9073486328125e-06\n",
      "tensor(4.4476e+08, device='cuda:0') 3.814697265625e-06\n",
      "tensor(4.4931e+08, device='cuda:0') 7.62939453125e-06\n",
      "tensor(4.4433e+08, device='cuda:0') 1.52587890625e-05\n",
      "tensor(4.4869e+08, device='cuda:0') 3.0517578125e-05\n",
      "tensor(4.4284e+08, device='cuda:0') 6.103515625e-05\n",
      "tensor(4.4477e+08, device='cuda:0') 0.0001220703125\n",
      "tensor(4.4750e+08, device='cuda:0') 0.000244140625\n",
      "tensor(4.4251e+08, device='cuda:0') 0.00048828125\n",
      "tensor(4.4618e+08, device='cuda:0') 0.0009765625\n",
      "tensor(4.4304e+08, device='cuda:0') 0.001953125\n",
      "tensor(4.4452e+08, device='cuda:0') 0.00390625\n",
      "tensor(4.4416e+08, device='cuda:0') 0.0078125\n",
      "tensor(4.4417e+08, device='cuda:0') 0.015625\n",
      "tensor(4.4514e+08, device='cuda:0') 0.03125\n",
      "tensor(4.4695e+08, device='cuda:0') 0.0625\n",
      "tensor(4.4673e+08, device='cuda:0') 0.125\n",
      "tensor(4.4361e+08, device='cuda:0') 0.25\n",
      "tensor(4.4930e+08, device='cuda:0') 0.5\n",
      "tensor(4.4639e+08, device='cuda:0') 1.0\n",
      "tensor(4.4244e+08, device='cuda:0') 2.0\n",
      "tensor(4.4485e+08, device='cuda:0') 4.0\n",
      "tensor(4.4677e+08, device='cuda:0') 8.0\n",
      "tensor(4.4293e+08, device='cuda:0') 16.0\n",
      "tensor(4.4496e+08, device='cuda:0') 32.0\n",
      "tensor(4.4556e+08, device='cuda:0') 64.0\n",
      "tensor(4.4454e+08, device='cuda:0') 128.0\n",
      "tensor(4.4249e+08, device='cuda:0') 256.0\n",
      "tensor(4.4419e+08, device='cuda:0') 512.0\n",
      "tensor(4.4583e+08, device='cuda:0') 1024.0\n",
      "tensor(4.4400e+08, device='cuda:0') 2048.0\n",
      "tensor(4.4247e+08, device='cuda:0') 4096.0\n",
      "tensor(4.4359e+08, device='cuda:0') 8192.0\n",
      "tensor(4.4452e+08, device='cuda:0') 16384.0\n",
      "tensor(4.4355e+08, device='cuda:0') 32768.0\n",
      "tensor(4.4250e+08, device='cuda:0') 65536.0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "dataloaders = {\n",
    "    'train':feature_loader,\n",
    "    'val': feature_loader\n",
    "    }\n",
    "num_class = 6\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "print(device)\n",
    "model = train_model(model, optimizer, exp_lr_scheduler, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3540067e49e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_arg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pred' is not defined"
     ]
    }
   ],
   "source": [
    "train_loss = model[1]\n",
    "val_loss = model[2]\n",
    "plt.plot(train_loss, label = 'train')\n",
    "plt.plot(val_loss, label = 'val')\n",
    "plt.xlabel('Batch number')\n",
    "plt.ylabel('Epoch number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unique(pred_arg, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, labels in dataloaders[\"train\"]:\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = model(inputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg = torch.argmax(outputs, dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unique(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
